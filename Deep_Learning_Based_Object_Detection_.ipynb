{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_Based_Object_Detection .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15Nfl5KbRmt-iMxa5vMbGEapv7kiiox2N",
      "authorship_tag": "ABX9TyPccqA3Uo+53K4eT9JfzCE3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sikandarmir/Object-Detection-/blob/main/Deep_Learning_Based_Object_Detection_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tikZWF2t0Lg"
      },
      "source": [
        "\n",
        "#Object Detection:\n",
        "Object detection is a computer vision technique that allows us to identify and locate objects in an image or video.\n",
        "\n",
        "**Deep Learning Based Object Detection**\n",
        "1.Architecture : Mobilenet Based Single shot Multi-Box(SSD)\n",
        "\n",
        "2:Framework : Tensorflow\n",
        "\n",
        "\n",
        "**Single Shot MultiBox Detector(SSD)**\n",
        "\n",
        "Link:https://www.youtube.com/watch?v=UwyzEejuGWk&ab_channel=CodeWithAarohi\n",
        "\n",
        "\n",
        "*   Single Shot: this means that the tasks of object localization and classification are done in a single forward pass of the network\n",
        "*   MultiBox: this is the name of a technique for bounding box regression developed by Szegedy et al. (we will briefly cover it shortly)\n",
        "\n",
        "*   The network is an object detector that also classifies those detected objects.\n",
        "\n",
        "**YOLO**\n",
        "YOLO is an abbreviation for the term ‘You Only Look Once’. This is an algorithm that detects and recognizes various objects in a picture (in real-time). Object detection in YOLO is done as a regression problem and provides the class probabilities of the detected images.\n",
        "\n",
        "YOLO algorithm employs convolutional neural networks (CNN) to detect objects in real-time. As the name suggests, the algorithm requires only a single forward propagation through a neural network to detect objects.\n",
        "\n",
        "Link:https://www.youtube.com/watch?v=ag3DLKsl2vk&ab_channel=codebasics\n",
        "In Yolo algorthuim the Image is X Train anad Y Train will be vector of size 7.and the out put it also give the vector of size 7 to tell that this is a particular object or not.  \n",
        "To Implement The Yolo Algorthuim follow the Follwing link instruction\n",
        "Link:https://github.com/AlexeyAB/darknet#how-to-compile-on-windows-using-cmake\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UR7DiUEBiIA"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Za0NQZnpAp",
        "outputId": "3f1daf0d-aba1-4fd3-f8c0-3e0cacf395e4"
      },
      "source": [
        "!curl -O http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!curl -O https://drive.google.com/file/d/121-kCXaOHOkJE_Kf5lKcJvC_5q1fYb_q/view"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0^C\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX_hp2zh0N5Y"
      },
      "source": [
        "#Unzip .tar file Extension\n",
        "import tarfile\n",
        "my_tar = tarfile.open('/content/VOCtrainval_06-Nov-2007.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Data_set') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mv4S8O8uGsk"
      },
      "source": [
        "# !pip install adam\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "import tensorflow\n",
        "import keras\n",
        "# from keras.optimizers import adam\n",
        "# # from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# from models.keras_ssd300 import ssd_300\n",
        "\n",
        "# from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
        "\n",
        "# from data_generator.object_detection_2d_data_generator import DataGenerator\n",
        "# from data_generator.object_detection_2d_geometric_ops import Resize\n",
        "# from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
        "# from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWKvcAtvkp1L"
      },
      "source": [
        "img_height = 300 \n",
        "img_width = 300 \n",
        "img_channels = 3 \n",
        "mean_color = [123, 117, 104] \n",
        "swap_channels = [2, 1, 0] #BGR instead of RGB\n",
        "n_classes = 20 \n",
        "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] \n",
        "scales = scales_pascal\n",
        "aspect_ratios = [[1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
        "                 [1.0, 2.0, 0.5],\n",
        "                 [1.0, 2.0, 0.5]] \n",
        "two_boxes_for_ar1 = True #used in anchorboxes.py. used for next_scale \n",
        "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
        "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
        "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
        "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
        "normalize_coords = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "Iw0hwBlm2Jax",
        "outputId": "9ae69225-f5bb-4176-9bd6-984e09094f30"
      },
      "source": [
        "\n",
        "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
        "\n",
        "\n",
        "# Parse the image and label lists for the training and validation datasets. This can take a while.\n",
        "VOC_2007_images_dir      = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/JPEGImages/'\n",
        "\n",
        "VOC_2007_annotations_dir      = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/Annotations/'\n",
        "\n",
        "# The paths to the image sets.\n",
        "VOC_2007_train_image_set_filename    = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/ImageSets/Main/train.txt'\n",
        "VOC_2007_val_image_set_filename      = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/ImageSets/Main/val.txt'\n",
        "\n",
        "VOC_2007_trainval_image_set_filename = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/ImageSets/Main/trainval.txt'\n",
        "VOC_2007_test_image_set_filename     = '/content/drive/MyDrive/Data_set/VOCdevkit/VOC2007/ImageSets/Main/test.txt'\n",
        "\n",
        "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
        "classes = ['background',\n",
        "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "           'bottle', 'bus', 'car', 'cat',\n",
        "           'chair', 'cow', 'diningtable', 'dog',\n",
        "           'horse', 'motorbike', 'person', 'pottedplant',\n",
        "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
        "                        image_set_filenames=[VOC_2007_trainval_image_set_filename],\n",
        "                        annotations_dirs=[VOC_2007_annotations_dir],\n",
        "                        classes=classes,\n",
        "                        include_classes='all',\n",
        "                        exclude_truncated=False,\n",
        "                        exclude_difficult=False,\n",
        "                        ret=False)\n",
        "\n",
        "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
        "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
        "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
        "                      classes=classes,\n",
        "                      include_classes='all',\n",
        "                      exclude_truncated=False,\n",
        "                      exclude_difficult=True,\n",
        "                      ret=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-99d8d7beef9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_images_into_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_dataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_images_into_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdf5_dataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataGenerator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmFkDDqJA4Zn"
      },
      "source": [
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ielfGduOBEMm"
      },
      "source": [
        "\n",
        "# Set the image transformations for pre-processing and data augmentation options.\n",
        "\n",
        "# For the training generator:\n",
        "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
        "                                            img_width=img_width,\n",
        "                                            background=mean_color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ1KecXRBGtV"
      },
      "source": [
        "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
        "                n_classes=n_classes,\n",
        "                mode='training',\n",
        "                l2_regularization=0.0005,\n",
        "                scales=scales,\n",
        "                aspect_ratios_per_layer=aspect_ratios,\n",
        "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                steps=steps,\n",
        "                offsets=offsets,\n",
        "                clip_boxes=clip_boxes,\n",
        "                variances=variances,\n",
        "                normalize_coords=normalize_coords,\n",
        "                subtract_mean=mean_color,\n",
        "                swap_channels=swap_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ynUklhABJxe"
      },
      "source": [
        "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
        "\n",
        "# model's predictor layers to create the anchor boxes.\n",
        "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
        "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
        "\n",
        "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
        "                                    img_width=img_width,\n",
        "                                    n_classes=n_classes,\n",
        "                                    predictor_sizes=predictor_sizes,\n",
        "                                    scales=scales,\n",
        "                                    aspect_ratios_per_layer=aspect_ratios,\n",
        "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
        "                                    steps=steps,\n",
        "                                    offsets=offsets,\n",
        "                                    clip_boxes=clip_boxes,\n",
        "                                    variances=variances,\n",
        "                                    matching_type='multi',\n",
        "                                    pos_iou_threshold=0.5,\n",
        "                                    neg_iou_limit=0.5,\n",
        "                                    normalize_coords=normalize_coords)\n",
        "\n",
        "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
        "train_generator = train_dataset.generate(batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         transformations=[ssd_data_augmentation],\n",
        "                                         label_encoder=ssd_input_encoder,\n",
        "                                         returns={'processed_images',\n",
        "                                                  'filenames',\n",
        "                                                  'inverse_transform',\n",
        "                                                  'original_images',\n",
        "                                                  'original_labels'},\n",
        "                                         keep_images_without_gt=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCYI3WkGBWjs"
      },
      "source": [
        "# number of samples in the training and validations datasets.\n",
        "train_dataset_size = train_dataset.get_dataset_size()\n",
        "val_dataset_size   = val_dataset.get_dataset_size()\n",
        "\n",
        "print(\"Number of images in the training dataset: \", train_dataset_size)\n",
        "print(\"Number of images in the validation dataset:  \", val_dataset_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCAyaFh5BXXd"
      },
      "source": [
        "# 2: Generate samples.\n",
        "\n",
        "batch_images, batch_filenames, batch_inverse_transforms, batch_original_images, batch_original_labels = next(train_generator)\n",
        "\n",
        "i = 0 # Which batch item to look at\n",
        "\n",
        "print(\"Image:\", batch_filenames[i])\n",
        "\n",
        "print(\"Ground truth boxes:\\n\")\n",
        "print(np.array(batch_original_labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7etLVoj4BZgd"
      },
      "source": [
        "# 5: Draw the predicted boxes onto the image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the colors for the bounding boxes\n",
        "colors = plt.cm.hsv(np.linspace(0, 1, n_classes+1)).tolist()\n",
        "classes = ['background',\n",
        "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "           'bottle', 'bus', 'car', 'cat',\n",
        "           'chair', 'cow', 'diningtable', 'dog',\n",
        "           'horse', 'motorbike', 'person', 'pottedplant',\n",
        "           'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "plt.imshow(batch_original_images[i])\n",
        "\n",
        "current_axis = plt.gca()\n",
        "\n",
        "#plt.plot()\n",
        "\n",
        "#plotting groundtrithbox\n",
        "for box in batch_original_labels[i]:\n",
        "    xmin = box[1]\n",
        "    ymin = box[2]\n",
        "    xmax = box[3]\n",
        "    ymax = box[4]\n",
        "    label = '{}'.format(classes[int(box[0])])\n",
        "    current_axis.add_patch(plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, color='green', fill=False, linewidth=2))  \n",
        "    current_axis.text(xmin, ymin, label, size='x-large', color='white', bbox={'facecolor':'green', 'alpha':1.0})"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}